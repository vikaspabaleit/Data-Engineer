1. How would you find duplicate records in a table and delete only the duplicates while keeping one copy?
2. A table has a start_date and end_date column. How would you calculate the total number of overlapping days across all rows?
3. How would you read a large CSV file in chunks, filter only required rows, and write the output into a new file using Python?
4. You have a nested dictionary of JSON records from an API. How would you flatten it and load it into a Pandas DataFrame?
5. How would you handle null values and incorrect casing in a PySpark DataFrame?
6. Write a PySpark logic to compute the difference in days between two status changes for each user.
7. How would you write a PySpark job to merge (upsert) data from a new file into an existing Delta table?
8. How can you dynamically copy multiple tables from an on-prem SQL Server to Azure Data Lake using ADF?
9. You need to delete files from multiple containers daily before writing new data. ADF doesn’t support wildcards across containers in one delete activity. How would you design this?
10. Explain how you would build a Medallion Architecture in Databricks to process daily sales data.
11. You need to track policy status changes over time. How would you use PySpark and Delta Lake to implement this in Databricks?
12. What are the key differences between Dedicated SQL Pool and Serverless SQL Pool in Synapse?
13. You are asked to ingest and transform 100GB of CSV files daily in Synapse. What is your approach?
